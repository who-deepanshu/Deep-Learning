
# Natural Language Processing
A machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.

## Tokenization 
- The process of converting a sequence of text into smaller parts, known as tokens. These tokens can be as small as characters or as long as words.

<p align="center">
  <img src="https://github.com/who-deepanshu/Deep-Learning/assets/129099978/9c026a0a-8b8b-451f-ad32-be342fe40a8c" width="600" height="370" </img>
</p>


## Stopwords
- To eliminate words that are so widely used that they carry very little useful information. 


## Stemming & Lemmatization
- Both aims to reduce inflections down to common base root words.

<p align="center">
  <img src="https://github.com/who-deepanshu/Deep-Learning/assets/129099978/174b9fc1-5570-467c-a519-c157a1efb34c"</img>
</p>

## Bag Of Words
## TD-IDF
## Word-2-Vec
 
