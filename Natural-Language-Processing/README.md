
# Natural Language Processing
A machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.

## Tokenization 
- The process of converting a sequence of text into smaller parts, known as tokens. These tokens can be as small as characters or as long as words.

<p align="center">
  <img src="https://github.com/who-deepanshu/Deep-Learning/assets/129099978/9c026a0a-8b8b-451f-ad32-be342fe40a8c" width="600" height="370" </img>
</p>


## Stopwords
- To eliminate words that are so widely used that they carry very little useful information. 


## Stemming & Lemmatization
- Both aims to reduce inflections down to common base root words.

<p align="center">
  <img src="https://github.com/who-deepanshu/Deep-Learning/assets/129099978/a88c54f0-9a69-4960-890d-47819b36a076"</img>
</p>

